{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531f0fd8",
   "metadata": {},
   "source": [
    "                                                üòÅ  Bem vindo! üòÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e55601",
   "metadata": {},
   "source": [
    "#### Meu nome √© Luis Trindade e s√≥cio fundador da empresa Iteratec Desenvolvimento de Software!\n",
    "\n",
    "#### Este notebook √© material complementar para o curso dispon√≠vel no youtube: https://www.youtube.com/playlist?list=PLhJbeYspP-A-Dj8Te8dUtf5Q9dprQuZk8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894e610",
   "metadata": {},
   "source": [
    "# Criando nossa base de dados üìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1497f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFile\n",
    "from random import random\n",
    "import shutil\n",
    "\n",
    "def build_dir(list_of_folders):\n",
    "    for item in path:\n",
    "        try:\n",
    "            os.listdir(f'../images/{item}')\n",
    "        except:\n",
    "            os.mkdir(f'../images/{item}')\n",
    "\n",
    "def create_dataset():\n",
    "    for file in os.listdir('../images'):\n",
    "        print(file)\n",
    "        if(file.endswith(\".png\")):\n",
    "            im=Image.open('../images/' + file)\n",
    "            rgb_im = im.convert('RGB')\n",
    "            imResize = rgb_im.resize((256,256), Image.ANTIALIAS)\n",
    "            imResize.save('../images/mod_'+file)  \n",
    "            rand=random()\n",
    "            if(rand<=0.85):\n",
    "                shutil.move('../images/mod_' + file,'../images/train/' + file)\n",
    "            else:\n",
    "                shutil.move('../images/mod_' + file,'../images/test/' + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56319c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=['train','test']\n",
    "build_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b040452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b5b21",
   "metadata": {},
   "source": [
    "# Preparando nosso dataset üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b716f2",
   "metadata": {},
   "source": [
    "### Fun√ß√µes Auxiliares üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63470f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando nossos pacotes e fun√ß√µes:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "#Preprocessor - One hot encoder --> Transforma uma vari√°vel discreta em m√∫ltiplas vari√°veis bin√°rias.\n",
    "def preprocessor_disease(df):\n",
    "    \n",
    "    categories=df.Disease.unique()\n",
    "    ohe = OneHotEncoder(drop='if_binary', sparse = False)\n",
    "    ohe.fit(df[['Disease']]) # Fit encoder\n",
    "    diseases_encoaded = ohe.transform(df[['Disease']])\n",
    "    categories.sort()\n",
    "    for index,item in enumerate(categories):\n",
    "        df[item]=diseases_encoaded[:,index]    \n",
    "    return df\n",
    "\n",
    "#Create csv file --> Cria um arquivo do tipo .csv contendo o nome do arquivo com e sem extens√£o e a doen√ßa presente\n",
    "#no t√≠tulo do arquivo.\n",
    "def create_data_csv(path, folder):\n",
    "    list_images=[]\n",
    "    for file in os.listdir(path):\n",
    "        list_images.append({\"filename\":os.path.splitext(file)[0],\"Disease\":(os.path.splitext(file)[0]).split(\"-\")[0]})\n",
    "    df=pd.DataFrame(list_images)\n",
    "    df=preprocessor_disease(df)\n",
    "    try:\n",
    "        df.drop(index=df.loc[df['filename']==folder+'_df'].index[0],inplace=True)\n",
    "        df.drop(columns=folder+'_df',inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    df.to_csv(path+ '/' + folder + '_df.csv', index=False)\n",
    "    \n",
    "#Plot distribution --> Cria um gr√°fico de barras contendo as categorias, ou, doen√ßas, no eixo X\n",
    "#e a contagem de ocorr√™ncias no eixo Y.    \n",
    "def plot_dist(df,categories,position,reference):  \n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    for index,item in enumerate(categories):\n",
    "        x_axis.append(item)\n",
    "        y_axis.append(df.loc[df[item]==1].count()[0])\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.barplot(x=x_axis,y=y_axis,ax=axs[position]).set(title=reference)\n",
    "    sns.lineplot(x=x_axis,y=np.average(y_axis),ax=axs[position])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9872e",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo o caminho das pastas train e test.\n",
    "path=['../images/train','../images/test']\n",
    "folder=['train','test']\n",
    "\n",
    "#Criando arquivos do tipo .csv utilizando a fun√ß√£o definida nas fun√ß√µes auxiliares.\n",
    "for index,item in enumerate(path):\n",
    "    create_data_csv(path[index],folder[index])\n",
    "    \n",
    "#Transformando o conte√∫do do arquivo .csv em um Pandas DataFrame.\n",
    "train_df=pd.read_csv(path[0]+ '/' + folder[0] + '_df.csv')\n",
    "test_df=pd.read_csv(path[1]+ '/' + folder[1] + '_df.csv')\n",
    "\n",
    "#Definindo uma porcentagem aproximada do DataFrame que ser√° utilizada no treinamento.\n",
    "n=0.1 #Para trabalharmos apenas com 10% do dataset\n",
    "\n",
    "#Realizando a abertura das imagens e transformando as mesmas em uma lista de matrizes.\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "for file in os.listdir('../images/train'):\n",
    "    rand=random()\n",
    "    if(rand<=0.1):\n",
    "        if(file.endswith(\".png\")):\n",
    "            im=Image.open('../images/train/' + file)\n",
    "            X_train.append(np.asarray(im))\n",
    "            im_row=train_df[train_df['filename']==os.path.splitext(file)[0]].drop(columns=\"filename\")\n",
    "            Y_train.append(im_row.drop(columns='Disease').to_numpy()[0])\n",
    "\n",
    "#Transformando a lista de matrizes em uma estrutura do tipo Numpy Array.\n",
    "X_train=np.asarray(X_train)\n",
    "#Normalizando as matrizes que comp√µem a imagem cujos valores encontram-se entre 0 e 255 para o intervalo de 0 e 1.\n",
    "X_train=X_train/255\n",
    "#Transformando nossa lista de listas em uma estrutura do tipo Numpy Array.\n",
    "Y_train=np.asarray(Y_train)\n",
    "\n",
    "#Embaralhando nosso ambiente de treinamento de forma a garantir que as doen√ßas estejam distribuidas ao acaso.\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "#Separando nosso ambiente de treinamento do ambiente de valida√ß√£o em 70% dos dados para treinamento e 30% para valida√ß√£o.\n",
    "div=math.floor(len(X_train)*0.7)\n",
    "X_tr = X_train[:div]\n",
    "y_tr = Y_train[:div]\n",
    "X_val = X_train[div:]\n",
    "y_val = Y_train[div:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ae81e",
   "metadata": {},
   "source": [
    "# Avaliando nosso dataset üîç "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d278d",
   "metadata": {},
   "source": [
    "### Fun√ß√µes Auxiliares üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot distribution --> Cria um gr√°fico de barras contendo as categorias, ou, doen√ßas, no eixo X\n",
    "#e a contagem de ocorr√™ncias no eixo Y.    \n",
    "def plot_dist(df,categories,position,reference):  \n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    for index,item in enumerate(categories):\n",
    "        x_axis.append(item)\n",
    "        y_axis.append(df.loc[df[item]==1].count()[0])\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.barplot(x=x_axis,y=y_axis,ax=axs[position]).set(title=reference)\n",
    "    sns.lineplot(x=x_axis,y=np.average(y_axis),ax=axs[position])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b43931",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add566a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "#Realizando apresenta√ß√µes gr√°ficas da nossa base de dados\n",
    "f, axs = plt.subplots(1,2,figsize=(12,4))    \n",
    "categories=['COVID','Lung_Opacity','Normal','Viral Pneumonia']\n",
    "plot_dist(train_df,categories,0,'Amostra Original')\n",
    "train_df=pd.DataFrame(Y_train,columns=['COVID','Lung_Opacity','Normal','Viral Pneumonia'])\n",
    "plot_dist(train_df,categories,1,'Amostra contendo '+str(n*100)+'% da amostra original')\n",
    "\n",
    "print(f\"Nosso baseline para a nova amostragem √© de aproximadamente {str(round(train_df.loc[train_df['Normal']==1].count()[0]/len(X_train)*100))}%.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce21966",
   "metadata": {},
   "source": [
    "# Definindo nosso modelo ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c340d5",
   "metadata": {},
   "source": [
    "### Fun√ß√µes Auxiliares üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Fun√ß√£o de inicializa√ß√£o do modelo, indicando as camadas do modelo.\n",
    "def initialize_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (2, 2), activation='relu', padding='same', input_shape=(256, 256, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))  \n",
    "    model.add(Conv2D(16, (2, 2), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))  \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "#Fun√ß√£o de compila√ß√£o do modelo\n",
    "def compile_model(model):\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = adam,\n",
    "                  metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892ca5c",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05237ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando diret√≥rio onde iremos salvar os pesos de nosso modelo durante o treinamento\n",
    "try:\n",
    "    os.listdir('models')\n",
    "    for file in os.listdir('models'):\n",
    "        os.remove('models/'+file)\n",
    "except:\n",
    "    os.mkdir(f'models')\n",
    "\n",
    "#Definindo a m√©trica de otimiza√ß√£o do modelo.\n",
    "metrics=['accuracy']\n",
    "\n",
    "#Definindo a taxa de aprendizado e o nosso otimizador.\n",
    "Learning_rate=0.01\n",
    "adam = Adam(learning_rate = Learning_rate)\n",
    "\n",
    "#Inicializando nosso modelo\n",
    "model= initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41a13c",
   "metadata": {},
   "source": [
    "# Treinando nosso modelo ü¶æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936ae9b",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fe3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando checkpoint para que os pesos do modelo sejam salvos a cada √©poca do treinamento.\n",
    "model_checkpoint=ModelCheckpoint('models/model{epoch:02d}.h5',save_weights_only=True)\n",
    "\n",
    "#Realizando a compila√ß√£o do modelo.\n",
    "model = compile_model(model)\n",
    "\n",
    "#Realizando o treinamento do nosso modelo.\n",
    "history = model.fit(x=X_tr, y=y_tr, \n",
    "                    validation_data=(X_val,y_val),\n",
    "                    use_multiprocessing=True,                    \n",
    "                    batch_size=16,\n",
    "                    epochs = 50, \n",
    "                    callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990897e9",
   "metadata": {},
   "source": [
    "# Avaliando a performance do nosso modelo üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da226d",
   "metadata": {},
   "source": [
    "### Fun√ß√µes Auxiliares üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Fun√ß√£o que ir√° realizar uma representa√ß√£o gr√°fica da curva de perda e acur√°cia do modelo.\n",
    "def plot_loss_accuracy_curve(history):\n",
    "    \n",
    "    fig, ax =plt.subplots(1,2,figsize=(20,5))\n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend(['Train', 'Val'], loc='upper right')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy', fontsize = 18)\n",
    "    ax[1].set_xlabel('Epoch', fontsize = 14)\n",
    "    ax[1].set_ylabel('Precision', fontsize = 14)\n",
    "    ax[1].legend(['Train', 'Val'], loc='lower right')  \n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#Fun√ß√£o que ir√° realizar uma representa√ß√£o gr√°fica da matriz de confus√£o.\n",
    "def plot_confusion_matrix(X,y,model):\n",
    "    Y_pred=model.predict(X)\n",
    "    Y_pred_cat=[]\n",
    "    Y_cat=[]\n",
    "    for item in Y_pred:\n",
    "        Y_pred_cat.append(np.argmax(item))\n",
    "    for item in y:\n",
    "        Y_cat.append(np.argmax(item))\n",
    "    \n",
    "    cm = confusion_matrix(Y_cat, Y_pred_cat,normalize='true')\n",
    "    cm_2 = confusion_matrix(Y_cat, Y_pred_cat)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = categories, \n",
    "                     columns = categories)\n",
    "\n",
    "    cm2_df = pd.DataFrame(cm_2,\n",
    "                     index = categories, \n",
    "                     columns = categories)\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm_df, annot=True)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm2_df, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actal Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(Y_cat,Y_pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e84794",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bfef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizando a representa√ß√£o gr√°fica das curvas de perda e acur√°cia durante o treinamento.\n",
    "plot_loss_accuracy_curve(history)\n",
    "\n",
    "#Realizando a representa√ß√£o gr√°fica da matriz de confus√£o para o treinamento e valida√ß√£o.\n",
    "plot_confusion_matrix(X_tr,y_tr,model)\n",
    "plot_confusion_matrix(X_val,y_val,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cdb91",
   "metadata": {},
   "source": [
    "# Salvando nosso modelo üíæ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0b9e8",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521dbf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "import glob\n",
    "\n",
    "#Transformando o modelo em um arquivo .json para salvar.\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    print(\"Modelo salvo com sucesso\")\n",
    "    \n",
    "#Carregando o modelo a partir de um arquivo do tipo .json.\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "#Carregando o peso da √∫ltima √©poca de treinamento do modelo.\n",
    "list_of_files = glob.glob('models/*')\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "try:\n",
    "    model.load_weights(latest_file)\n",
    "    print(\"Modelo carregado com sucesso\")\n",
    "    print(f\"Peso carregado est√° no diret√≥rio {latest_file}\")\n",
    "    compile_model(model)\n",
    "except:\n",
    "    print('Verifique se existe hist√≥rico de treinamento para o modelo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92baec09",
   "metadata": {},
   "source": [
    "# Avaliando a performance do nosso modelo com imagens de teste üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8b747",
   "metadata": {},
   "source": [
    "### Fun√ß√µes Auxiliares üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba113bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "\n",
    "#Fun√ß√£o para cria√ß√£o de representa√ß√£o gr√°fica da curva de ROC.\n",
    "def plot_roc_curve(fpr,tpr): \n",
    "    plt.plot(fpr,tpr) \n",
    "    x = np.arange(0, 1.1, 0.1).tolist()\n",
    "    y = x\n",
    "    plt.plot(x,y)\n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf559cd4",
   "metadata": {},
   "source": [
    "### Processamento principalüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "Y_test=[]\n",
    "\n",
    "#Criando dataset de teste para avalia√ß√£o do modelo em ambiente de teste.\n",
    "for file in os.listdir('images/test'):\n",
    "    if(file.endswith(\".png\")):\n",
    "        im=Image.open('images/test/' + file)\n",
    "        X_test.append(np.asarray(im))\n",
    "        im_row=test_df[test_df['filename']==os.path.splitext(file)[0]].drop(columns=\"filename\")\n",
    "        Y_test.append(im_row.drop(columns='Disease').to_numpy()[0])\n",
    "        \n",
    "X_test=np.asarray(X_test)\n",
    "X_test=X_test/255\n",
    "Y_test=np.asarray(Y_test)\n",
    "\n",
    "res = model.evaluate(X_test, Y_test, verbose = 1)\n",
    "\n",
    "#Aferindo a acur√°cia do modelo em dados de teste.\n",
    "print(f'Acur√°cia do modelo para os dados do diret√≥rio de teste:  {res[1]*100:.2f}%')\n",
    "\n",
    "#Realizando representa√ß√£o gr√°fica da matriz de confus√£o das predi√ß√µes do modelo com dados de teste.\n",
    "plot_confusion_matrix(X_test,Y_test,model)\n",
    "\n",
    "#Realizando representa√ß√£o gr√°fica das curvas de ROC\n",
    "f, axs = plt.subplots(2,4,figsize=(20,10))\n",
    "bins = [i/20 for i in range(20)] + [1]\n",
    "classes = categories\n",
    "roc_auc_ovr = {}\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    # Gets the class\n",
    "    c = classes[i]\n",
    "    \n",
    "    df_aux=pd.DataFrame(Y_pred,columns=categories).join(test_df['Disease'])\n",
    "    # Prepares an auxiliar dataframe to help with the plots\n",
    "    df_aux[\"class\"] = [1 if y == c else 0 for y in df_aux['Disease']]\n",
    "    df_aux['prob'] = df_aux[c]\n",
    "\n",
    "    # Plots the probability distribution for the class and the rest\n",
    "    ax = plt.subplot(2, 4, i+1)\n",
    "    sns.histplot(x = \"prob\", data = df_aux, hue = 'class', color = 'b', ax = ax, bins = bins)\n",
    "    ax.set_title(c)\n",
    "    ax.legend([f\"Class: {c}\", \"Rest\"])\n",
    "    ax.set_xlabel(f\"P(x = {c})\")\n",
    "    \n",
    "    # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "    ax = plt.subplot(2, 4, i+5)\n",
    "    fpr , tpr , thresholds = roc_curve (df_aux['class'], df_aux['prob'])\n",
    "    plt.plot(fpr,tpr) \n",
    "    x = np.arange(0, 1.1, 0.1).tolist()\n",
    "    y = x\n",
    "    plt.plot(x,y)\n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    ax.set_title(\"ROC Curve OvR\")\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "    print(f\"AUC for {c} is {roc_auc_ovr[c]}.\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8939ae6302d706547273c597dd3be7d36d3b4c2b9aa367c9a420125c13753bf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
